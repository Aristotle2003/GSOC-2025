{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30f29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PATHOLOGY IMAGE GENERATION COMPARISON: VAE vs GAN vs Diffusion\n",
      "Running on device: cuda:0\n",
      "======================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "Successfully loaded UNI features (torch.Size([400, 1536])) and labels (torch.Size([400])).\n",
      "Successfully loaded real images from /extra/zhanglab0/INDV/zihend1/Disentanglement/ICIAR2018_BACH_Challenge/Photos.\n",
      "\n",
      "[2/6] Training VAE model...\n",
      "--- Training VAE for 50 epochs ---\n",
      "VAE Epoch 10/50, Average Loss: 0.6011\n",
      "VAE Epoch 20/50, Average Loss: 0.2787\n",
      "VAE Epoch 30/50, Average Loss: 0.1596\n",
      "VAE Epoch 40/50, Average Loss: 0.1029\n",
      "VAE Epoch 50/50, Average Loss: 0.0720\n",
      "\n",
      "[3/6] Training GAN model...\n",
      "--- Training GAN for 100 epochs ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comparing VAE, GAN, and Diffusion Models for Medical Image Synthesis\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.linalg import sqrtm\n",
    "from einops import rearrange\n",
    "\n",
    "# ==============================================================================\n",
    "# IMPORTANT: Add PathLDM to path BEFORE any ldm imports\n",
    "# The user needs to set this path correctly.\n",
    "# ==============================================================================\n",
    "PATH_LDM_PROJECT_ROOT = '/home/zihend1/Disentanglement/PathLDM'\n",
    "if PATH_LDM_PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PATH_LDM_PROJECT_ROOT)\n",
    "\n",
    "# Diffusion model imports (must be after sys.path update)\n",
    "from omegaconf import OmegaConf\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  centrally managed configuration\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    # --- System ---\n",
    "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    SEED = 42\n",
    "    \n",
    "    # --- Paths ---\n",
    "    # Diffusion Model\n",
    "    DIFFUSION_CONFIG_PATH = Path(PATH_LDM_PROJECT_ROOT) / \"plip_imagenet_finetune/configs/08-03T09-35-project.yaml\"\n",
    "    DIFFUSION_CKPT_PATH = Path(PATH_LDM_PROJECT_ROOT) / \"plip_imagenet_finetune/checkpoints/epoch_3.ckpt\"\n",
    "    \n",
    "    # VAE/GAN Training Data\n",
    "    UNI_FEATURES_PATH = \"uni_features.npy\"\n",
    "    UNI_LABELS_PATH = \"uni_labels.npy\"\n",
    "    REAL_IMAGES_PATH = '/extra/zhanglab0/INDV/zihend1/Disentanglement/ICIAR2018_BACH_Challenge/Photos'\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    BATCH_SIZE = 16\n",
    "    VAE_EPOCHS = 50\n",
    "    GAN_EPOCHS = 100\n",
    "    VAE_LR = 1e-4\n",
    "    GAN_G_LR = 1e-4\n",
    "    GAN_D_LR = 2e-5\n",
    "    BETA = 4.0 # For Beta-VAE\n",
    "    \n",
    "    # --- Model & Generation Parameters ---\n",
    "    LATENT_DIM = 32\n",
    "    CONDITION_DIM = 4\n",
    "    IMG_CHANNELS = 3\n",
    "    IMG_SIZE = 224 # VAE/GAN output size\n",
    "    DIFFUSION_GEN_SIZE = 64 # Native size of diffusion model\n",
    "    NUM_SAMPLES_PER_CLASS = 4\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# Model Definitions (VAE, GAN)\n",
    "# ==============================================================================\n",
    "\n",
    "class UNIBetaVAE(nn.Module):\n",
    "    \"\"\"Conditional Beta-VAE for pathology image generation\"\"\"\n",
    "    def __init__(self, uni_emb_dim=1536, latent_dim=Config.LATENT_DIM, condition_dim=Config.CONDITION_DIM, img_channels=Config.IMG_CHANNELS, img_size=Config.IMG_SIZE):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.fc_mu = nn.Linear(uni_emb_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(uni_emb_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        # Calculate initial size for ConvTranspose2d based on final desired size\n",
    "        # 224 -> 112 -> 56 -> 28 -> 14. We start from 14x14.\n",
    "        self.decoder_input = nn.Linear(latent_dim + condition_dim, 1024 * 14 * 14)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (1024, 14, 14)),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # 14->28\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), # 28->56\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 56->112\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # 112->224\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid() # Output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def encode(self, uni_emb):\n",
    "        mu = self.fc_mu(uni_emb)\n",
    "        logvar = self.fc_logvar(uni_emb)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, cond):\n",
    "        z_cond = torch.cat([z, cond], dim=1)\n",
    "        return self.decoder(self.decoder_input(z_cond))\n",
    "\n",
    "    def forward(self, uni_emb, cond):\n",
    "        mu, logvar = self.encode(uni_emb)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_img = self.decode(z, cond)\n",
    "        return recon_img, mu, logvar\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"GAN Generator, similar architecture to VAE Decoder\"\"\"\n",
    "    def __init__(self, latent_dim=Config.LATENT_DIM, condition_dim=Config.CONDITION_DIM, img_channels=Config.IMG_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 1024 * 14 * 14),\n",
    "            nn.Unflatten(1, (1024, 14, 14)),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, img_channels, 3, 1, 1),\n",
    "            nn.Tanh() # Output in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, cond):\n",
    "        input_tensor = torch.cat([z, cond], dim=1)\n",
    "        return self.model(input_tensor)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"GAN Discriminator with spectral normalization\"\"\"\n",
    "    def __init__(self, img_channels=Config.IMG_CHANNELS, condition_dim=Config.CONDITION_DIM, img_size=Config.IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(img_channels + condition_dim, 64, 4, 2, 1)), nn.LeakyReLU(0.2), # 224->112\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1)), nn.LeakyReLU(0.2), # 112->56\n",
    "            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1)), nn.LeakyReLU(0.2), # 56->28\n",
    "            spectral_norm(nn.Conv2d(256, 512, 4, 2, 1)), nn.LeakyReLU(0.2), # 28->14\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Calculate flattened size dynamically\n",
    "        final_feature_size = img_size // (2**4)\n",
    "        flattened_dim = 512 * final_feature_size * final_feature_size\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, 1),\n",
    "            nn.Sigmoid() # Probability output\n",
    "        )\n",
    "\n",
    "    def forward(self, img, cond):\n",
    "        # Create a condition map and concatenate it as a channel\n",
    "        cond = cond.view(cond.size(0), cond.size(1), 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        input_tensor = torch.cat([img, cond], dim=1)\n",
    "        features = self.model(input_tensor)\n",
    "        return self.output_layer(features)\n",
    "        \n",
    "# ==============================================================================\n",
    "# Diffusion Model Wrapper\n",
    "# ==============================================================================\n",
    "class DiffusionModelWrapper:\n",
    "    \"\"\"Fixed wrapper for the pre-trained Latent Diffusion Model\"\"\"\n",
    "    def __init__(self, config_path, ckpt_path, device):\n",
    "        self.device = device\n",
    "        self.available = False\n",
    "        self.model = None\n",
    "        self.sampler = None\n",
    "        self.label_text = {\n",
    "            0: \"Normal tissue\",\n",
    "            1: \"Benign tumor tissue\",\n",
    "            2: \"In-situ carcinoma\",\n",
    "            3: \"Invasive carcinoma\"\n",
    "        }\n",
    "        \n",
    "        if not config_path.exists() or not ckpt_path.exists():\n",
    "            print(f\"Warning: Diffusion model paths not found.\")\n",
    "            print(f\"  Config path: {config_path}\")\n",
    "            print(f\"  Checkpoint path: {ckpt_path}\")\n",
    "            print(\"Diffusion model will be unavailable.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print(f\"Loading diffusion model from {ckpt_path}...\")\n",
    "            config = OmegaConf.load(config_path)\n",
    "            \n",
    "            # As you correctly did, remove ckpt_path from config to avoid conflicts\n",
    "            config.model.params.first_stage_config.params.pop('ckpt_path', None)\n",
    "            config.model.params.unet_config.params.pop('ckpt_path', None)\n",
    "            \n",
    "            pl_sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            sd = pl_sd.get(\"state_dict\", pl_sd)\n",
    "            \n",
    "            self.model = instantiate_from_config(config.model)\n",
    "            self.model.load_state_dict(sd, strict=False)\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            self.sampler = DDIMSampler(self.model)\n",
    "            self.available = True\n",
    "            print(\"Diffusion model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading diffusion model: {e}\")\n",
    "            print(\"Diffusion model will be unavailable.\")\n",
    "            self.available = False\n",
    "\n",
    "    def generate_images(self, labels, num_samples_per_class, steps=50, scale=1.5, image_size=Config.DIFFUSION_GEN_SIZE):\n",
    "        \"\"\"Generate images using the diffusion model\"\"\"\n",
    "        if not self.available:\n",
    "            print(\"Cannot generate images: Diffusion model is not available.\")\n",
    "            return None, None\n",
    "        \n",
    "        expanded_labels = [label for label in labels for _ in range(num_samples_per_class)]\n",
    "        batch_size = len(expanded_labels)\n",
    "        prompts = [self.label_text[i] for i in expanded_labels]\n",
    "        \n",
    "        shape = [Config.IMG_CHANNELS, image_size, image_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get unconditional and conditional embeddings\n",
    "            uc = self.model.get_learned_conditioning([\"\"] * batch_size)\n",
    "            cc = self.model.get_learned_conditioning(prompts)\n",
    "            \n",
    "            # Sample from the model\n",
    "            samples_ddim, _ = self.sampler.sample(\n",
    "                S=steps,\n",
    "                batch_size=batch_size,\n",
    "                shape=shape,\n",
    "                conditioning=cc,\n",
    "                verbose=False,\n",
    "                unconditional_guidance_scale=scale,\n",
    "                unconditional_conditioning=uc,\n",
    "                eta=0.0\n",
    "            )\n",
    "            \n",
    "            # Decode latents to pixel space and normalize to [0, 1]\n",
    "            x_samples = self.model.decode_first_stage(samples_ddim)\n",
    "            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            \n",
    "            # Upsample to match VAE/GAN output size for fair comparison\n",
    "            x_samples = F.interpolate(x_samples, size=(Config.IMG_SIZE, Config.IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return x_samples, expanded_labels\n",
    "\n",
    "# ==============================================================================\n",
    "# Training and Data Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def get_dataloaders(device):\n",
    "    \"\"\"Loads real data or creates synthetic data as a fallback.\"\"\"\n",
    "    try:\n",
    "        # Attempt to load pre-computed features and labels\n",
    "        uni_emb = torch.tensor(np.load(Config.UNI_FEATURES_PATH)).float()\n",
    "        labels = torch.tensor(np.load(Config.UNI_LABELS_PATH)).long()\n",
    "        labels_onehot = F.one_hot(labels, num_classes=Config.CONDITION_DIM).float()\n",
    "        print(f\"Successfully loaded UNI features ({uni_emb.shape}) and labels ({labels.shape}).\")\n",
    "        \n",
    "        uni_dataset = TensorDataset(uni_emb.to(device), labels_onehot.to(device))\n",
    "        uni_loader = DataLoader(uni_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: UNI feature/label files not found. Creating synthetic data for VAE/GAN training.\")\n",
    "        uni_emb = torch.randn(1000, 1536).to(device)\n",
    "        labels = torch.randint(0, Config.CONDITION_DIM, (1000,)).to(device)\n",
    "        labels_onehot = F.one_hot(labels, num_classes=Config.CONDITION_DIM).float().to(device)\n",
    "        uni_dataset = TensorDataset(uni_emb, labels_onehot)\n",
    "        uni_loader = DataLoader(uni_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    try:\n",
    "        # Attempt to load real images\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset_img = datasets.ImageFolder(root=Config.REAL_IMAGES_PATH, transform=transform)\n",
    "        loader_img = DataLoader(dataset_img, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Successfully loaded real images from {Config.REAL_IMAGES_PATH}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load real images from path. Reason: {e}\")\n",
    "        print(\"Creating synthetic images for VAE/GAN training.\")\n",
    "        synthetic_imgs = torch.rand(1000, Config.IMG_CHANNELS, Config.IMG_SIZE, Config.IMG_SIZE).to(device)\n",
    "        synthetic_labels = torch.zeros(1000) # Dummy labels\n",
    "        img_dataset = TensorDataset(synthetic_imgs, synthetic_labels)\n",
    "        loader_img = DataLoader(img_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "    return uni_loader, loader_img\n",
    "\n",
    "def train_vae(uni_loader, loader_img, device):\n",
    "    \"\"\"Train VAE model\"\"\"\n",
    "    print(f\"--- Training VAE for {Config.VAE_EPOCHS} epochs ---\")\n",
    "    model = UNIBetaVAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.VAE_LR)\n",
    "    \n",
    "    for epoch in range(Config.VAE_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, batch_count = 0, 0\n",
    "        \n",
    "        for (uni_batch, cond_batch), (img_batch, _) in zip(uni_loader, loader_img):\n",
    "            recon_img, mu, logvar = model(uni_batch, cond_batch)\n",
    "            \n",
    "            recon_loss = F.mse_loss(recon_img, img_batch.to(device))\n",
    "            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            kl_div /= uni_batch.size(0) # Per-sample KL\n",
    "            loss = recon_loss + Config.BETA * kl_div\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_loss = total_loss / batch_count\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"VAE Epoch {epoch+1}/{Config.VAE_EPOCHS}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def train_gan(uni_loader, loader_img, device):\n",
    "    \"\"\"Train GAN model\"\"\"\n",
    "    print(f\"--- Training GAN for {Config.GAN_EPOCHS} epochs ---\")\n",
    "    gen = Generator().to(device)\n",
    "    disc = Discriminator().to(device)\n",
    "    \n",
    "    opt_g = optim.Adam(gen.parameters(), lr=Config.GAN_G_LR, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(disc.parameters(), lr=Config.GAN_D_LR, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(Config.GAN_EPOCHS):\n",
    "        epoch_g_loss, epoch_d_loss, batch_count = 0, 0, 0\n",
    "        \n",
    "        for (uni_batch, cond_batch), (real_img, _) in zip(uni_loader, loader_img):\n",
    "            real_img = real_img.to(device) * 2.0 - 1.0 # Normalize to [-1, 1] for Tanh\n",
    "            batch_size = real_img.size(0)\n",
    "            \n",
    "            # --- Train Discriminator ---\n",
    "            opt_d.zero_grad()\n",
    "            real_labels = torch.ones(batch_size, 1).to(device) * 0.9 # Label smoothing\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device) + 0.1 # Label smoothing\n",
    "            \n",
    "            pred_real = disc(real_img, cond_batch)\n",
    "            loss_d_real = criterion(pred_real, real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, Config.LATENT_DIM).to(device)\n",
    "            fake_img = gen(z, cond_batch)\n",
    "            pred_fake = disc(fake_img.detach(), cond_batch)\n",
    "            loss_d_fake = criterion(pred_fake, fake_labels)\n",
    "            \n",
    "            loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            # --- Train Generator ---\n",
    "            opt_g.zero_grad()\n",
    "            pred_g = disc(fake_img, cond_batch)\n",
    "            loss_g = criterion(pred_g, real_labels) # Trick discriminator\n",
    "            loss_g.backward()\n",
    "            opt_g.step()\n",
    "            \n",
    "            epoch_g_loss += loss_g.item()\n",
    "            epoch_d_loss += loss_d.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_g_loss = epoch_g_loss / batch_count\n",
    "        avg_d_loss = epoch_d_loss / batch_count\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"GAN Epoch [{epoch+1}/{Config.GAN_EPOCHS}] | D_loss: {avg_d_loss:.4f} | G_loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "    return gen, disc\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Image Quality Evaluation\n",
    "# ==============================================================================\n",
    "class ImageQualityEvaluator:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.inception = models.inception_v3(pretrained=True, transform_input=False)\n",
    "        self.inception.fc = nn.Identity()\n",
    "        self.inception.to(device).eval()\n",
    "    \n",
    "    def calculate_fid(self, real_features, fake_features):\n",
    "        mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "        mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "        diff = mu1 - mu2\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        if np.iscomplexobj(covmean): covmean = covmean.real\n",
    "        fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n",
    "        return fid\n",
    "\n",
    "    def calculate_is(self, images, splits=10):\n",
    "        N = len(images)\n",
    "        preds = []\n",
    "        for i in range(0, N, Config.BATCH_SIZE):\n",
    "            batch = images[i:i+Config.BATCH_SIZE].to(self.device)\n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            with torch.no_grad():\n",
    "                pred = F.softmax(self.inception(batch), dim=1)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "        preds = np.concatenate(preds, 0)\n",
    "        scores = []\n",
    "        for i in range(splits):\n",
    "            part = preds[i * (N // splits): (i + 1) * (N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores_part = []\n",
    "            for k in range(part.shape[0]):\n",
    "                pyx = part[k, :]\n",
    "                scores_part.append(F.kl_div(torch.log(torch.tensor(pyx)), torch.tensor(py), reduction='sum').item())\n",
    "            scores.append(np.exp(np.mean(scores_part)))\n",
    "        return np.mean(scores), np.std(scores)\n",
    "\n",
    "    def calculate_ssim(self, real_images, fake_images):\n",
    "        ssim_scores = []\n",
    "        for real, fake in zip(real_images, fake_images):\n",
    "            real_np = real.cpu().numpy().transpose(1, 2, 0)\n",
    "            fake_np = fake.cpu().numpy().transpose(1, 2, 0)\n",
    "            real_gray = cv2.cvtColor((real_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            fake_gray = cv2.cvtColor((fake_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            score = ssim(real_gray, fake_gray, data_range=255)\n",
    "            ssim_scores.append(score)\n",
    "        return np.mean(ssim_scores)\n",
    "\n",
    "    def extract_features(self, images):\n",
    "        features = []\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), Config.BATCH_SIZE):\n",
    "                batch = images[i:i+Config.BATCH_SIZE].to(self.device)\n",
    "                batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "                batch = normalize(batch)\n",
    "                feat = self.inception(batch)\n",
    "                features.append(feat.cpu().numpy())\n",
    "        return np.concatenate(features, 0)\n",
    "\n",
    "    def evaluate(self, generated_images, real_images=None):\n",
    "        results = {}\n",
    "        generated_images = torch.clamp(generated_images, 0, 1)\n",
    "        try:\n",
    "            is_mean, is_std = self.calculate_is(generated_images)\n",
    "            results['IS_mean'], results['IS_std'] = is_mean, is_std\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate IS: {e}\")\n",
    "            results['IS_mean'], results['IS_std'] = 'N/A', 'N/A'\n",
    "        \n",
    "        if real_images is not None:\n",
    "            real_images = torch.clamp(real_images, 0, 1)\n",
    "            try:\n",
    "                real_features = self.extract_features(real_images)\n",
    "                fake_features = self.extract_features(generated_images)\n",
    "                results['FID'] = self.calculate_fid(real_features, fake_features)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate FID: {e}\")\n",
    "                results['FID'] = 'N/A'\n",
    "            try:\n",
    "                min_len = min(len(real_images), len(generated_images))\n",
    "                results['SSIM'] = self.calculate_ssim(real_images[:min_len], generated_images[:min_len])\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate SSIM: {e}\")\n",
    "                results['SSIM'] = 'N/A'\n",
    "        return results\n",
    "\n",
    "# ==============================================================================\n",
    "# Visualization Functions\n",
    "# ==============================================================================\n",
    "def visualize_comparison(image_dict):\n",
    "    label_text = {0: \"Normal\", 1: \"Benign\", 2: \"In-situ\", 3: \"Invasive\"}\n",
    "    num_methods = len(image_dict)\n",
    "    num_classes = Config.CONDITION_DIM\n",
    "    \n",
    "    fig, axes = plt.subplots(num_methods, num_classes, figsize=(12, 3 * num_methods))\n",
    "    fig.suptitle('Pathology Generation: Model Comparison', fontsize=16)\n",
    "\n",
    "    for row, (method, data) in enumerate(image_dict.items()):\n",
    "        images, labels = data['images'], data['labels']\n",
    "        if images is None:\n",
    "            continue\n",
    "            \n",
    "        axes[row, 0].set_ylabel(method, fontsize=14, rotation=90, labelpad=20)\n",
    "        \n",
    "        for col in range(num_classes):\n",
    "            # Find the first image for the current class\n",
    "            try:\n",
    "                idx = labels.index(col)\n",
    "                img = images[idx].cpu().permute(1, 2, 0).numpy()\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                axes[row, col].imshow(img)\n",
    "                if row == 0:\n",
    "                    axes[row, col].set_title(label_text[col])\n",
    "            except (ValueError, IndexError):\n",
    "                # Handle case where a class might not have a generated image\n",
    "                axes[row, col].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Pipeline\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PATHOLOGY IMAGE GENERATION COMPARISON: VAE vs GAN vs Diffusion\")\n",
    "    print(f\"Running on device: {Config.DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # --- Step 1: Load Data ---\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    uni_loader, loader_img = get_dataloaders(Config.DEVICE)\n",
    "    if not uni_loader or not loader_img:\n",
    "        print(\"Fatal: Could not load or create data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Train VAE and GAN ---\n",
    "    print(\"\\n[2/6] Training VAE model...\")\n",
    "    vae_model = train_vae(uni_loader, loader_img, Config.DEVICE)\n",
    "    \n",
    "    print(\"\\n[3/6] Training GAN model...\")\n",
    "    gan_generator, _ = train_gan(uni_loader, loader_img, Config.DEVICE)\n",
    "\n",
    "    # --- Step 4: Load Diffusion Model ---\n",
    "    print(\"\\n[4/6] Loading pre-trained Diffusion model...\")\n",
    "    diffusion_wrapper = DiffusionModelWrapper(\n",
    "        Config.DIFFUSION_CONFIG_PATH, Config.DIFFUSION_CKPT_PATH, Config.DEVICE\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Generate Images ---\n",
    "    print(\"\\n[5/6] Generating images from all models...\")\n",
    "    all_generated_images = {}\n",
    "    \n",
    "    # VAE Generation\n",
    "    vae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(Config.NUM_SAMPLES_PER_CLASS * Config.CONDITION_DIM, Config.LATENT_DIM).to(Config.DEVICE)\n",
    "        labels = torch.tensor([i for i in range(Config.CONDITION_DIM) for _ in range(Config.NUM_SAMPLES_PER_CLASS)])\n",
    "        condition = F.one_hot(labels, num_classes=Config.CONDITION_DIM).float().to(Config.DEVICE)\n",
    "        vae_images = vae_model.decode(z, condition)\n",
    "        all_generated_images['VAE'] = {'images': vae_images, 'labels': labels.tolist()}\n",
    "        print(f\"Generated {len(vae_images)} images from VAE.\")\n",
    "\n",
    "    # GAN Generation\n",
    "    gan_generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Using the same z and labels for a more direct comparison\n",
    "        gan_images_raw = gan_generator(z, condition)\n",
    "        gan_images = (gan_images_raw + 1.0) / 2.0 # Rescale from [-1, 1] to [0, 1]\n",
    "        all_generated_images['GAN'] = {'images': gan_images, 'labels': labels.tolist()}\n",
    "        print(f\"Generated {len(gan_images)} images from GAN.\")\n",
    "\n",
    "    # Diffusion Generation\n",
    "    diffusion_images, diffusion_labels = diffusion_wrapper.generate_images(\n",
    "        labels=list(range(Config.CONDITION_DIM)),\n",
    "        num_samples_per_class=Config.NUM_SAMPLES_PER_CLASS\n",
    "    )\n",
    "    all_generated_images['Diffusion'] = {'images': diffusion_images, 'labels': diffusion_labels}\n",
    "    if diffusion_images is not None:\n",
    "        print(f\"Generated {len(diffusion_images)} images from Diffusion model.\")\n",
    "\n",
    "    # --- Step 6: Visualize and Evaluate ---\n",
    "    print(\"\\n[6/6] Visualizing results and running evaluation...\")\n",
    "    visualize_comparison(all_generated_images)\n",
    "\n",
    "    # Prepare real images for evaluation\n",
    "    real_images_for_eval = []\n",
    "    for i, (img_batch, _) in enumerate(loader_img):\n",
    "        real_images_for_eval.append(img_batch)\n",
    "        if len(real_images_for_eval) * Config.BATCH_SIZE >= 100: # Gather ~100 images\n",
    "            break\n",
    "    real_images_for_eval = torch.cat(real_images_for_eval, dim=0) if real_images_for_eval else None\n",
    "\n",
    "    evaluator = ImageQualityEvaluator(Config.DEVICE)\n",
    "    \n",
    "    for method, data in all_generated_images.items():\n",
    "        print(f\"\\n--- Evaluating {method} ---\")\n",
    "        if data['images'] is None:\n",
    "            print(\"No images to evaluate.\")\n",
    "            continue\n",
    "        \n",
    "        results = evaluator.evaluate(data['images'], real_images_for_eval)\n",
    "        print(f\"  Inception Score (IS): {results.get('IS_mean', 'N/A')}\")\n",
    "        print(f\"  Fréchet Inception Distance (FID): {results.get('FID', 'N/A')}\")\n",
    "        print(f\"  Structural Similarity (SSIM): {results.get('SSIM', 'N/A')}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoomldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
